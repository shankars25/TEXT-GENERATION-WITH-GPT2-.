#  GPT-2 Text Generation Project

###  Prodigy Infotech Internship Task – Task 1 (Comillas Negras)
**Author:** Shankar Sutar
**Project:** Text Generation using GPT-2  

---

## Objective

To develop and train a model capable of generating fluent and contextually accurate text based on a given prompt using **GPT-2**, a transformer-based language model developed by **OpenAI**.

---

##Project Description

This project demonstrates the **fine-tuning** and **text generation** process using the GPT-2 model.  
By learning from a custom dataset, the model can generate realistic and human-like sentences that resemble the style of the training data.  
Once fine-tuned, it can automatically produce meaningful and coherent continuations for any given starting prompt.

This work is completed as part of **Prodigy Infotech – Task 1 (Comillas Negras)**.

---

## Key Features

-  Uses **GPT-2**, a pre-trained transformer model from OpenAI  
-  Supports **fine-tuning** on custom text datasets  
-  Generates coherent text continuations from user input prompts  
-  Simple to run on **Google Colab** or a local environment  

---

## How to Run

1. Open the notebook **`Task_01_GPT2_Text_Generation.ipynb`** in **Google Colab**.  
2. Install the required libraries using:
   ```bash
   pip install transformers torch datasets
